import json
import random
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import ollama
from sklearn.metrics.pairwise import cosine_similarity
from langchain.docstore.document import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms.base import LLM
from codecarbon import EmissionsTracker
import subprocess
tracker = EmissionsTracker(project_name="PatentRAG")

app = FastAPI(title="RAG pour le syst√®me d'√©ducation aux brevets avec Ollama & LangChain")

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:5173", "http://localhost:8000"],
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)

#########################################
# 1. Charger le vector store avec BGE-M3
#########################################
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

vectorstore = FAISS.load_local(
    "faiss_index",
    embedding_model,
    allow_dangerous_deserialization=True
)

print(f"[INFO] Index FAISS charg√© avec {len(vectorstore.docstore._dict)} documents.")

#########################################
# 2. D√©finir un LLM personnalis√© utilisant Ollama
#########################################
class OllamaLLM(LLM):
    @property
    def _llm_type(self) -> str:
        return "ollama"

    def _call(self, prompt: str, stop: list = None) -> str:
        response = ollama.generate(model='mistral', prompt=prompt)

        try:
            return response.get("response", "")
        except Exception:
            return str(response)

    def predict(self, prompt: str, stop: list = None) -> str:
        return self._call(prompt, stop=stop)

ollama_llm = OllamaLLM()


#########################################
# 3. Endpoints FastAPI
#########################################
@app.get("/generate-question")
def generate_question(category: str):
    tracker.start()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 60})
    all_docs = retriever.get_relevant_documents(f"about {category}")
    random.shuffle(all_docs)

    # S√©lection majoritaire d'examens
    exam_docs = [doc for doc in all_docs if doc.metadata.get("type", "").lower() == "exam"][:30]
    # Ajouter d'autres types pour enrichir le contexte
    other_docs = [
        doc for doc in all_docs
        if doc.metadata.get("type", "").lower() in ["law", "treaty", "guideline"]
    ][:15]

    selected_docs = exam_docs + other_docs
    if not selected_docs:
        selected_docs = all_docs[:20]

    context = "\n\n".join([doc.page_content[:600] for doc in selected_docs])

    formulations = [
        "G√©n√©rez une seule question d'examen √† choix multiples",
        "Cr√©ez une seule question d'examen ouverte et stimulante",
        "Produisez une question d'examen unique ayant une pertinence juridique",
        "R√©digez une question unique bas√©e sur un sc√©nario en droit des brevets"
    ]
    formulation = random.choice(formulations)

    prompt = f"""
-- D√âBUT DU CONTEXTE (documents officiels et supports d'examen) --
{context}
-- FIN DU CONTEXTE --

En fran√ßais, g√©n√©rez une seule question concise en droit europ√©en des brevets sur le th√®me '{category}'. {formulation}.
Ne fournissez pas la r√©ponse. Retournez uniquement la question et assurez-vous d'en g√©n√©rer une seule.
"""

    try:
        question = ollama_llm.predict(prompt)
        tracker.start()
        return {"question": question.strip()}
    except Exception as e:
        return {"error": str(e)}

    
# === G√©n√©ration de question al√©atoire ===
@app.get("/generate-random-question")
def generate_random_question():
    retriever = vectorstore.as_retriever(search_kwargs={"k": 60})
    all_docs = retriever.get_relevant_documents("")
    random.shuffle(all_docs)

    exam_docs = [doc for doc in all_docs if doc.metadata.get("type") == "exam"][:30]
    other_docs = [doc for doc in all_docs if doc.metadata.get("type") in ["law", "treaty", "guideline"]][:15]

    selected_docs = exam_docs + other_docs
    context = "\n\n".join([doc.page_content[:600] for doc in selected_docs])

    formulations = [
        "G√©n√©rez une seule question d'examen √† choix multiples",
        "Cr√©ez une seule question bas√©e sur un sc√©nario ouvert",
        "G√©n√©rez une question d'examen r√©aliste en droit europ√©en des brevets"
    ]
    formulation = random.choice(formulations)

    prompt = f"""
-- CONTEXTE : Documents en droit europ√©en des brevets (EPC, PCT, Directives, Examens) --
{context}
-- FIN DU CONTEXTE --

{formulation}. En fran√ßais.Ne fournissez aucune r√©ponse. Retournez uniquement une question, et seulement une.
"""

    try:
        question = ollama_llm.predict(prompt)
        return {"question": question.strip()}
    except Exception as e:
        return {"error": str(e)}


# Re-ranking pour l'analyse de r√©ponses : on donne plus d'importance aux documents proches de la question mais on booste ceux qui sont proches des deux
def rerank_docs(question, answer, vectorstore, embedding_model, top_k_question=20, top_k_final=10):
    # Embedding des requ√™tes
    q_vec = embedding_model.embed_query(question)
    a_vec = embedding_model.embed_query(answer)

    # R√©cup√©ration initiale par la question
    initial_docs = vectorstore.similarity_search(question, k=top_k_question)

    # Pour chaque doc, calcul de similarit√© avec Q et A
    scored_docs = []
    for doc in initial_docs:
        doc_vec = embedding_model.embed_query(doc.page_content)
        sim_q = cosine_similarity([q_vec], [doc_vec])[0][0]
        sim_a = cosine_similarity([a_vec], [doc_vec])[0][0]

        # Score mixte pond√©r√© : plus important de coller √† la question
        final_score = 0.7 * sim_q + 0.3 * sim_a
        scored_docs.append((final_score, doc))

    # Tri + s√©lection top-k
    top_docs = [doc for score, doc in sorted(scored_docs, reverse=True)[:top_k_final]]
    return top_docs


class AnalyzeRequest(BaseModel):
    category: str
    user_question: str
    user_answer: str

@app.post("/analyze-answer")
def analyze_answer(request: AnalyzeRequest):
    top_docs = rerank_docs(
        question=request.user_question,
        answer=request.user_answer,
        vectorstore=vectorstore,
        embedding_model=embedding_model,
        top_k_question=20,
        top_k_final=10
    )

    law_docs = [doc for doc in top_docs if doc.metadata.get("type") in ["law", "treaty", "guideline"]]
    exam_docs = [doc for doc in top_docs if doc.metadata.get("type") == "exam"]

    law_context = "\n\n".join([doc.page_content[:1000] for doc in law_docs])
    exam_context = "\n\n".join([doc.page_content[:1000] for doc in exam_docs])

    prompt = f"""
Vous √™tes un examinateur juridique sp√©cialis√© en droit europ√©en des brevets (EPC). Votre r√¥le est d'√©valuer les r√©ponses des √©tudiants dans le contexte des examens en droit europ√©en des brevets. Vous disposez de textes juridiques officiels ainsi que de questions d'examen avec leurs r√©ponses mod√®les pour vous aider dans votre √©valuation.

-- CONTEXTE ISSU DES TEXTES JURIDIQUES OFFICIELS (EPC, PCT, DIRECTIVES) --
{law_context}
-- FIN DU CONTEXTE JURIDIQUE --

-- EXEMPLES DE QUESTIONS D'EXAMENS PR√âC√âDENTS AVEC LEURS R√âPONSES --
{exam_context}
-- FIN DES EXEMPLES --

-- ENTR√âE DE L'√âTUDIANT --
Question : {request.user_question}
R√©ponse : {request.user_answer}
-- FIN DE L'ENTR√âE DE L'√âTUDIANT --

-- T√ÇCHE --
√âvaluez la r√©ponse de l'√©tudiant dans le style d'un correcteur d'examens professionnel.

Si c'est une question √† choix multiples, d√©terminez d'abord si l'option s√©lectionn√©e par l'√©tudiant est juridiquement correcte, notez que l'√©tudiant **n'est pas tenu** de fournir une justification ‚Äî votre r√¥le est de confirmer ou infirmer l'option s√©lectionn√©e **en vous basant sur la pr√©cision juridique**, et de fournir le raisonnement correct.

Pour les autres types de questions, votre r√©ponse doit inclure :

1. ‚úÖ - **√âvaluation Juridique** : La r√©ponse s√©lectionn√©e est-elle correcte ? Indiquez clairement si elle est juste ou fausse et pourquoi.
2. üí¨ - **Retour Constructif** : Si la r√©ponse est incorrecte ou incompl√®te, expliquez ce qui manque √† l'√©tudiant et comment s'am√©liorer.
3. üìù - **R√©ponse Mod√®le** : Fournissez une r√©ponse compl√®te et juridiquement solide, telle qu'attendue dans un examen.
4. üìö - **Fondement Juridique Cit√©** : Citez des articles, r√®gles ou Directives sp√©cifiques de l'EPC ou du PCT qui √©tayent votre √©valuation.

Si la r√©ponse de l'√©tudiant est vide ou hors sujet, fournissez uniquement la R√©ponse Mod√®le et le Fondement Juridique Cit√©.

**En fran√ßais.Gardez votre analyse rigoureuse mais utile. Vous √™tes √† la fois un p√©dagogue et un juriste**.
"""
    try:
        response = ollama_llm.predict(prompt)
        return {"feedback": response.strip()}
    except Exception as e:
        return {"error": str(e)}

# === G√©n√©ration de r√©ponse mod√®le uniquement ===
class ModelAnswerRequest(BaseModel):
    user_question: str
@app.post("/generate-model-answer")
def generate_model_answer(request: ModelAnswerRequest):
    top_docs = rerank_docs(
        question=request.user_question,
        answer="",  # Pas de r√©ponse √©tudiante
        vectorstore=vectorstore,
        embedding_model=embedding_model,
        top_k_question=20,
        top_k_final=10
    )

    law_docs = [doc for doc in top_docs if doc.metadata.get("type") in ["law", "treaty", "guideline"]]
    exam_docs = [doc for doc in top_docs if doc.metadata.get("type") == "exam"]

    law_context = "\n\n".join([doc.page_content[:1000] for doc in law_docs])
    exam_context = "\n\n".join([doc.page_content[:1000] for doc in exam_docs])

    prompt = f"""
Vous √™tes un expert juridique en droit europ√©en des brevets. √Ä partir de la question, des documents juridiques et des examens pr√©c√©dents ci-dessous, fournissez uniquement une r√©ponse mod√®le juridiquement solide ainsi que le fondement juridique pertinent.

-- CONTEXTE ISSU DES TEXTES JURIDIQUES OFFICIELS (EPC, PCT, DIRECTIVES) --
{law_context}
-- FIN DU CONTEXTE JURIDIQUE --

-- EXEMPLES D'EXAMENS PR√âC√âDENTS --
{exam_context}
-- FIN DES EXEMPLES --

-- ENTR√âE DE L'√âTUDIANT (AUCUNE R√âPONSE FOURNIE) --
Question : {request.user_question}
-- FIN DE L'ENTR√âE --

Fournissez :
üìù R√©ponse Mod√®le
üìö Fondement Juridique
"""
    try:
        response = ollama_llm.predict(prompt)
        return {"feedback": response.strip()}
    except Exception as e:
        return {"error": str(e)}
    

@app.get("/retrieve")
def retrieve(query: str, k: int = 11):
    docs = vectorstore.similarity_search(query, k=k)

    results = [
        {
            "source": doc.metadata.get("source"),
            "type": doc.metadata.get("type"),
            "excerpt": doc.page_content[:2000]
        }
        for doc in docs
    ]
    return {"results": results}

#########################################
# 4. Lancer l'application
#########################################
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)
    tracker.stop()
    print(tracker.emissions)